#!/bin/bash
# Start the Dask cluster on the lip/crunch machines

# Don't tolerate errors
set -e

# Use this commented-out part to also set up Hadoop with Dask
# # Configuration variables
# export HADOOP_CONF_DIR=/home/slerique/Code/hadoop/hadoop-2.9.0/etc/hadoop
#
# # Get the list of active (non-excluded) slaves
# ACTIVE_SLAVES=$(mktemp)
# cat $HADOOP_CONF_DIR/slaves | grep -v -Ff $HADOOP_CONF_DIR/exclude > $ACTIVE_SLAVES
#
# # Start the cluster
# HADOOP_CONF_DIR=/home/slerique/Code/hadoop/hadoop-2.9.0/etc/hadoop dask-ssh --scheduler $(hostname) --hostfile $ACTIVE_SLAVES --log-directory /datastore/complexnet/nw2vec/dask-cluster/log
#
# # Clean up
# rm $ACTIVE_SLAVES

#dask-ssh --scheduler $(hostname) --hostfile dask-slaves --log-directory $(pwd)/data/dask-cluster/log
cat dask-slaves | grep -v '^#' | sed 's/ *#.*$//g' | while read slave_nprocs; do
  slave_nprocs=($slave_nprocs)
  slave=${slave_nprocs[0]}
  nprocs=${slave_nprocs[1]}
  echo "Starting slave '$slave' with $nprocs processes"
  ssh $slave "killall -u \$(whoami) dask-worker; . \$HOME/anaconda3/etc/profile.d/conda.sh; conda activate base36; screen -A -m -d -S dask -L $(pwd)/data/dask-cluster/log/$slave nice -n 19 dask-worker --nprocs $nprocs $(hostname):8786"
done

echo "All done.
