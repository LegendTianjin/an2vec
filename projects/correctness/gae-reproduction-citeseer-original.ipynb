{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce original VGAE (Kipf & Welling 2016) Cora embeddings\n",
    "\n",
    "## Run the original training code (corrected to run inside Jupyter)\n",
    "\n",
    "It trains embeddings using the `adj_train` adjacency, where ~15% of the edges have been removed (10% for a test set, 5% for a validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from gae.input_data import load_data\n",
    "from gae.model import GCNModelAE, GCNModelVAE\n",
    "from gae.preprocessing import preprocess_graph, construct_feed_dict, sparse_to_tuple, mask_test_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options and move to necessary directory for data loading\n",
    "MODEL_STRING = 'gcn_vae'\n",
    "os.chdir('../../gae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 32, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 16, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('weight_decay', 0., 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "\n",
    "flags.DEFINE_string('model', MODEL_STRING, 'Model string.')\n",
    "flags.DEFINE_string('dataset', 'citeseer', 'Dataset string.')\n",
    "flags.DEFINE_integer('features', 1, 'Whether to use features (1) or not (0).')\n",
    "\n",
    "# Workaround for Jupyter having an '-f' flag in its sys.argv -- this is ignored\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = FLAGS.model\n",
    "dataset_str = FLAGS.dataset\n",
    "\n",
    "# Load data\n",
    "adj, features = load_data(dataset_str)\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "if FLAGS.features == 0:\n",
    "    features = sp.identity(features.shape[0])  # featureless\n",
    "\n",
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'features': tf.sparse_placeholder(tf.float32),\n",
    "    'adj': tf.sparse_placeholder(tf.float32),\n",
    "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=())\n",
    "}\n",
    "\n",
    "num_nodes = adj.shape[0]\n",
    "\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = None\n",
    "if model_str == 'gcn_ae':\n",
    "    model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "elif model_str == 'gcn_vae':\n",
    "    model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    if model_str == 'gcn_ae':\n",
    "        opt = OptimizerAE(preds=model.reconstructions,\n",
    "                          labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                      validate_indices=False), [-1]),\n",
    "                          pos_weight=pos_weight,\n",
    "                          norm=norm)\n",
    "    elif model_str == 'gcn_vae':\n",
    "        opt = OptimizerVAE(preds=model.reconstructions,\n",
    "                           labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                       validate_indices=False), [-1]),\n",
    "                           model=model, num_nodes=num_nodes,\n",
    "                           pos_weight=pos_weight,\n",
    "                           norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_score(edges_pos, edges_neg, emb=None):\n",
    "    if emb is None:\n",
    "        feed_dict.update({placeholders['dropout']: 0})\n",
    "        emb = sess.run(model.z_mean, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "acc_val = []\n",
    "val_roc_score = []\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.73741 train_acc= 0.49796 val_roc= 0.61854 val_ap= 0.64434 time= 2.60381\n",
      "Epoch: 0002 train_loss= 1.43449 train_acc= 0.46155 val_roc= 0.62835 val_ap= 0.66619 time= 0.37947\n",
      "Epoch: 0003 train_loss= 1.20442 train_acc= 0.42560 val_roc= 0.64696 val_ap= 0.69361 time= 0.75563\n",
      "Epoch: 0004 train_loss= 1.02252 train_acc= 0.39231 val_roc= 0.68767 val_ap= 0.73140 time= 0.78977\n",
      "Epoch: 0005 train_loss= 0.86154 train_acc= 0.37355 val_roc= 0.75783 val_ap= 0.77902 time= 0.97244\n",
      "Epoch: 0006 train_loss= 0.74621 train_acc= 0.39023 val_roc= 0.79796 val_ap= 0.81930 time= 0.78120\n",
      "Epoch: 0007 train_loss= 0.67363 train_acc= 0.39526 val_roc= 0.81377 val_ap= 0.83499 time= 0.58651\n",
      "Epoch: 0008 train_loss= 0.62895 train_acc= 0.39088 val_roc= 0.82371 val_ap= 0.84231 time= 0.56865\n",
      "Epoch: 0009 train_loss= 0.59811 train_acc= 0.41525 val_roc= 0.84294 val_ap= 0.85963 time= 0.40663\n",
      "Epoch: 0010 train_loss= 0.57273 train_acc= 0.41655 val_roc= 0.85183 val_ap= 0.86229 time= 0.44762\n",
      "Epoch: 0011 train_loss= 0.57051 train_acc= 0.40343 val_roc= 0.86126 val_ap= 0.87125 time= 0.39516\n",
      "Epoch: 0012 train_loss= 0.55487 train_acc= 0.43071 val_roc= 0.86761 val_ap= 0.87793 time= 0.40433\n",
      "Epoch: 0013 train_loss= 0.53767 train_acc= 0.48304 val_roc= 0.87085 val_ap= 0.88052 time= 0.51471\n",
      "Epoch: 0014 train_loss= 0.53436 train_acc= 0.50128 val_roc= 0.87186 val_ap= 0.88196 time= 0.36315\n",
      "Epoch: 0015 train_loss= 0.53606 train_acc= 0.49407 val_roc= 0.87345 val_ap= 0.88347 time= 0.56432\n",
      "Epoch: 0016 train_loss= 0.53868 train_acc= 0.48370 val_roc= 0.87366 val_ap= 0.88329 time= 0.60517\n",
      "Epoch: 0017 train_loss= 0.52821 train_acc= 0.49061 val_roc= 0.87574 val_ap= 0.88549 time= 0.53969\n",
      "Epoch: 0018 train_loss= 0.51431 train_acc= 0.50074 val_roc= 0.87721 val_ap= 0.88772 time= 0.39935\n",
      "Epoch: 0019 train_loss= 0.50577 train_acc= 0.50535 val_roc= 0.87747 val_ap= 0.88914 time= 0.38237\n",
      "Epoch: 0020 train_loss= 0.49824 train_acc= 0.50685 val_roc= 0.87617 val_ap= 0.88866 time= 0.51560\n",
      "Epoch: 0021 train_loss= 0.49773 train_acc= 0.50136 val_roc= 0.87423 val_ap= 0.88781 time= 0.38026\n",
      "Epoch: 0022 train_loss= 0.49772 train_acc= 0.49523 val_roc= 0.87361 val_ap= 0.88859 time= 0.51082\n",
      "Epoch: 0023 train_loss= 0.49184 train_acc= 0.50321 val_roc= 0.87217 val_ap= 0.88801 time= 0.64539\n",
      "Epoch: 0024 train_loss= 0.48607 train_acc= 0.51366 val_roc= 0.87180 val_ap= 0.88904 time= 0.48046\n",
      "Epoch: 0025 train_loss= 0.48341 train_acc= 0.51575 val_roc= 0.87137 val_ap= 0.88967 time= 0.81738\n",
      "Epoch: 0026 train_loss= 0.48533 train_acc= 0.50666 val_roc= 0.87141 val_ap= 0.89083 time= 0.89022\n",
      "Epoch: 0027 train_loss= 0.48603 train_acc= 0.50007 val_roc= 0.87378 val_ap= 0.89281 time= 0.65561\n",
      "Epoch: 0028 train_loss= 0.48075 train_acc= 0.50663 val_roc= 0.87751 val_ap= 0.89527 time= 0.51947\n",
      "Epoch: 0029 train_loss= 0.47488 train_acc= 0.51536 val_roc= 0.88069 val_ap= 0.89723 time= 0.40534\n",
      "Epoch: 0030 train_loss= 0.47276 train_acc= 0.51728 val_roc= 0.88366 val_ap= 0.89983 time= 0.51857\n",
      "Epoch: 0031 train_loss= 0.47082 train_acc= 0.51552 val_roc= 0.88587 val_ap= 0.90138 time= 0.51909\n",
      "Epoch: 0032 train_loss= 0.47025 train_acc= 0.51195 val_roc= 0.88791 val_ap= 0.90284 time= 0.37780\n",
      "Epoch: 0033 train_loss= 0.46776 train_acc= 0.51449 val_roc= 0.89012 val_ap= 0.90413 time= 0.53053\n",
      "Epoch: 0034 train_loss= 0.46456 train_acc= 0.51995 val_roc= 0.89113 val_ap= 0.90488 time= 0.61715\n",
      "Epoch: 0035 train_loss= 0.46308 train_acc= 0.52250 val_roc= 0.89181 val_ap= 0.90468 time= 0.52197\n",
      "Epoch: 0036 train_loss= 0.46326 train_acc= 0.52083 val_roc= 0.89332 val_ap= 0.90513 time= 0.39920\n",
      "Epoch: 0037 train_loss= 0.46328 train_acc= 0.52009 val_roc= 0.89375 val_ap= 0.90521 time= 0.39546\n",
      "Epoch: 0038 train_loss= 0.46211 train_acc= 0.52110 val_roc= 0.89425 val_ap= 0.90499 time= 0.42041\n",
      "Epoch: 0039 train_loss= 0.46047 train_acc= 0.52306 val_roc= 0.89522 val_ap= 0.90537 time= 0.38537\n",
      "Epoch: 0040 train_loss= 0.45977 train_acc= 0.52400 val_roc= 0.89557 val_ap= 0.90593 time= 0.47898\n",
      "Epoch: 0041 train_loss= 0.45873 train_acc= 0.52401 val_roc= 0.89594 val_ap= 0.90643 time= 0.44951\n",
      "Epoch: 0042 train_loss= 0.45841 train_acc= 0.52296 val_roc= 0.89484 val_ap= 0.90537 time= 0.39650\n",
      "Epoch: 0043 train_loss= 0.45733 train_acc= 0.52309 val_roc= 0.89352 val_ap= 0.90401 time= 0.60330\n",
      "Epoch: 0044 train_loss= 0.45578 train_acc= 0.52367 val_roc= 0.89321 val_ap= 0.90414 time= 0.59190\n",
      "Epoch: 0045 train_loss= 0.45463 train_acc= 0.52535 val_roc= 0.89350 val_ap= 0.90468 time= 0.97732\n",
      "Epoch: 0046 train_loss= 0.45416 train_acc= 0.52479 val_roc= 0.89326 val_ap= 0.90425 time= 0.60618\n",
      "Epoch: 0047 train_loss= 0.45370 train_acc= 0.52365 val_roc= 0.89290 val_ap= 0.90396 time= 0.56793\n",
      "Epoch: 0048 train_loss= 0.45328 train_acc= 0.52397 val_roc= 0.89225 val_ap= 0.90331 time= 0.65886\n",
      "Epoch: 0049 train_loss= 0.45189 train_acc= 0.52437 val_roc= 0.89286 val_ap= 0.90340 time= 0.55783\n",
      "Epoch: 0050 train_loss= 0.45101 train_acc= 0.52531 val_roc= 0.89328 val_ap= 0.90425 time= 0.48071\n",
      "Epoch: 0051 train_loss= 0.45080 train_acc= 0.52542 val_roc= 0.89301 val_ap= 0.90417 time= 0.40505\n",
      "Epoch: 0052 train_loss= 0.45043 train_acc= 0.52534 val_roc= 0.89229 val_ap= 0.90326 time= 0.58470\n",
      "Epoch: 0053 train_loss= 0.44957 train_acc= 0.52485 val_roc= 0.89216 val_ap= 0.90343 time= 0.62881\n",
      "Epoch: 0054 train_loss= 0.44879 train_acc= 0.52641 val_roc= 0.89231 val_ap= 0.90348 time= 0.47528\n",
      "Epoch: 0055 train_loss= 0.44818 train_acc= 0.52677 val_roc= 0.89202 val_ap= 0.90347 time= 0.40617\n",
      "Epoch: 0056 train_loss= 0.44782 train_acc= 0.52639 val_roc= 0.89222 val_ap= 0.90432 time= 0.39742\n",
      "Epoch: 0057 train_loss= 0.44721 train_acc= 0.52670 val_roc= 0.89235 val_ap= 0.90459 time= 0.47754\n",
      "Epoch: 0058 train_loss= 0.44632 train_acc= 0.52765 val_roc= 0.89237 val_ap= 0.90472 time= 0.42597\n",
      "Epoch: 0059 train_loss= 0.44571 train_acc= 0.52771 val_roc= 0.89315 val_ap= 0.90586 time= 0.38034\n",
      "Epoch: 0060 train_loss= 0.44539 train_acc= 0.52834 val_roc= 0.89330 val_ap= 0.90659 time= 0.39393\n",
      "Epoch: 0061 train_loss= 0.44497 train_acc= 0.52831 val_roc= 0.89338 val_ap= 0.90740 time= 0.45146\n",
      "Epoch: 0062 train_loss= 0.44395 train_acc= 0.52822 val_roc= 0.89291 val_ap= 0.90757 time= 0.50663\n",
      "Epoch: 0063 train_loss= 0.44356 train_acc= 0.52844 val_roc= 0.89299 val_ap= 0.90790 time= 0.62608\n",
      "Epoch: 0064 train_loss= 0.44270 train_acc= 0.52762 val_roc= 0.89373 val_ap= 0.90946 time= 0.60762\n",
      "Epoch: 0065 train_loss= 0.44226 train_acc= 0.52827 val_roc= 0.89359 val_ap= 0.90974 time= 1.11530\n",
      "Epoch: 0066 train_loss= 0.44211 train_acc= 0.52770 val_roc= 0.89383 val_ap= 0.91077 time= 0.92347\n",
      "Epoch: 0067 train_loss= 0.44159 train_acc= 0.52786 val_roc= 0.89371 val_ap= 0.91079 time= 0.67040\n",
      "Epoch: 0068 train_loss= 0.44106 train_acc= 0.52804 val_roc= 0.89427 val_ap= 0.91162 time= 0.50474\n",
      "Epoch: 0069 train_loss= 0.44069 train_acc= 0.52744 val_roc= 0.89412 val_ap= 0.91235 time= 0.64434\n",
      "Epoch: 0070 train_loss= 0.44025 train_acc= 0.52724 val_roc= 0.89437 val_ap= 0.91254 time= 0.58337\n",
      "Epoch: 0071 train_loss= 0.43987 train_acc= 0.52759 val_roc= 0.89383 val_ap= 0.91228 time= 0.69218\n",
      "Epoch: 0072 train_loss= 0.43944 train_acc= 0.52798 val_roc= 0.89408 val_ap= 0.91265 time= 0.53443\n",
      "Epoch: 0073 train_loss= 0.43904 train_acc= 0.52839 val_roc= 0.89385 val_ap= 0.91255 time= 0.37475\n",
      "Epoch: 0074 train_loss= 0.43895 train_acc= 0.52741 val_roc= 0.89369 val_ap= 0.91273 time= 0.38198\n",
      "Epoch: 0075 train_loss= 0.43848 train_acc= 0.52858 val_roc= 0.89369 val_ap= 0.91263 time= 0.37465\n",
      "Epoch: 0076 train_loss= 0.43802 train_acc= 0.52848 val_roc= 0.89319 val_ap= 0.91262 time= 0.39816\n",
      "Epoch: 0077 train_loss= 0.43792 train_acc= 0.52826 val_roc= 0.89290 val_ap= 0.91240 time= 0.38408\n",
      "Epoch: 0078 train_loss= 0.43776 train_acc= 0.52803 val_roc= 0.89299 val_ap= 0.91231 time= 0.49559\n",
      "Epoch: 0079 train_loss= 0.43702 train_acc= 0.52843 val_roc= 0.89208 val_ap= 0.91200 time= 0.43918\n",
      "Epoch: 0080 train_loss= 0.43734 train_acc= 0.52801 val_roc= 0.89210 val_ap= 0.91191 time= 0.39153\n",
      "Epoch: 0081 train_loss= 0.43722 train_acc= 0.52747 val_roc= 0.89134 val_ap= 0.91151 time= 0.44333\n",
      "Epoch: 0082 train_loss= 0.43642 train_acc= 0.52796 val_roc= 0.89140 val_ap= 0.91180 time= 0.60413\n",
      "Epoch: 0083 train_loss= 0.43691 train_acc= 0.52747 val_roc= 0.89119 val_ap= 0.91182 time= 0.61241\n",
      "Epoch: 0084 train_loss= 0.43659 train_acc= 0.52729 val_roc= 0.89117 val_ap= 0.91150 time= 0.41604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0085 train_loss= 0.43604 train_acc= 0.52731 val_roc= 0.89062 val_ap= 0.91097 time= 0.58285\n",
      "Epoch: 0086 train_loss= 0.43626 train_acc= 0.52722 val_roc= 0.89057 val_ap= 0.91155 time= 0.64926\n",
      "Epoch: 0087 train_loss= 0.43571 train_acc= 0.52798 val_roc= 0.89064 val_ap= 0.91182 time= 0.84282\n",
      "Epoch: 0088 train_loss= 0.43559 train_acc= 0.52833 val_roc= 0.89053 val_ap= 0.91123 time= 0.61343\n",
      "Epoch: 0089 train_loss= 0.43561 train_acc= 0.52818 val_roc= 0.89045 val_ap= 0.91106 time= 0.74100\n",
      "Epoch: 0090 train_loss= 0.43522 train_acc= 0.52788 val_roc= 0.89080 val_ap= 0.91111 time= 0.67112\n",
      "Epoch: 0091 train_loss= 0.43524 train_acc= 0.52839 val_roc= 0.89053 val_ap= 0.91087 time= 0.59541\n",
      "Epoch: 0092 train_loss= 0.43499 train_acc= 0.52914 val_roc= 0.89051 val_ap= 0.91058 time= 0.40283\n",
      "Epoch: 0093 train_loss= 0.43461 train_acc= 0.52887 val_roc= 0.89084 val_ap= 0.91105 time= 0.38043\n",
      "Epoch: 0094 train_loss= 0.43461 train_acc= 0.52909 val_roc= 0.89057 val_ap= 0.91051 time= 0.37621\n",
      "Epoch: 0095 train_loss= 0.43424 train_acc= 0.52904 val_roc= 0.89134 val_ap= 0.91010 time= 0.48432\n",
      "Epoch: 0096 train_loss= 0.43456 train_acc= 0.52831 val_roc= 0.89092 val_ap= 0.91027 time= 0.38502\n",
      "Epoch: 0097 train_loss= 0.43396 train_acc= 0.52981 val_roc= 0.89107 val_ap= 0.91035 time= 0.38945\n",
      "Epoch: 0098 train_loss= 0.43415 train_acc= 0.52881 val_roc= 0.89113 val_ap= 0.90972 time= 0.37527\n",
      "Epoch: 0099 train_loss= 0.43431 train_acc= 0.52850 val_roc= 0.89128 val_ap= 0.91030 time= 0.44375\n",
      "Epoch: 0100 train_loss= 0.43396 train_acc= 0.52855 val_roc= 0.89068 val_ap= 0.90967 time= 0.37090\n",
      "Epoch: 0101 train_loss= 0.43378 train_acc= 0.52798 val_roc= 0.89115 val_ap= 0.90990 time= 0.70090\n",
      "Epoch: 0102 train_loss= 0.43335 train_acc= 0.52923 val_roc= 0.89078 val_ap= 0.91014 time= 0.72065\n",
      "Epoch: 0103 train_loss= 0.43382 train_acc= 0.52945 val_roc= 0.89084 val_ap= 0.90998 time= 0.68229\n",
      "Epoch: 0104 train_loss= 0.43348 train_acc= 0.52886 val_roc= 0.89039 val_ap= 0.91004 time= 0.64102\n",
      "Epoch: 0105 train_loss= 0.43311 train_acc= 0.52909 val_roc= 0.89169 val_ap= 0.91076 time= 0.78840\n",
      "Epoch: 0106 train_loss= 0.43351 train_acc= 0.52911 val_roc= 0.89257 val_ap= 0.91187 time= 0.72431\n",
      "Epoch: 0107 train_loss= 0.43232 train_acc= 0.53025 val_roc= 0.89140 val_ap= 0.91094 time= 0.86202\n",
      "Epoch: 0108 train_loss= 0.43335 train_acc= 0.52898 val_roc= 0.89216 val_ap= 0.91166 time= 0.83149\n",
      "Epoch: 0109 train_loss= 0.43243 train_acc= 0.53074 val_roc= 0.89210 val_ap= 0.91125 time= 0.55402\n",
      "Epoch: 0110 train_loss= 0.43221 train_acc= 0.53099 val_roc= 0.89218 val_ap= 0.91158 time= 0.40117\n",
      "Epoch: 0111 train_loss= 0.43249 train_acc= 0.53016 val_roc= 0.89237 val_ap= 0.91231 time= 0.51198\n",
      "Epoch: 0112 train_loss= 0.43180 train_acc= 0.53149 val_roc= 0.89251 val_ap= 0.91216 time= 0.38457\n",
      "Epoch: 0113 train_loss= 0.43211 train_acc= 0.53134 val_roc= 0.89187 val_ap= 0.91183 time= 0.38797\n",
      "Epoch: 0114 train_loss= 0.43189 train_acc= 0.53105 val_roc= 0.89233 val_ap= 0.91220 time= 0.38221\n",
      "Epoch: 0115 train_loss= 0.43157 train_acc= 0.53122 val_roc= 0.89276 val_ap= 0.91276 time= 0.39654\n",
      "Epoch: 0116 train_loss= 0.43152 train_acc= 0.53169 val_roc= 0.89245 val_ap= 0.91315 time= 0.41306\n",
      "Epoch: 0117 train_loss= 0.43145 train_acc= 0.53259 val_roc= 0.89237 val_ap= 0.91317 time= 0.45749\n",
      "Epoch: 0118 train_loss= 0.43124 train_acc= 0.53169 val_roc= 0.89231 val_ap= 0.91349 time= 0.38659\n",
      "Epoch: 0119 train_loss= 0.43109 train_acc= 0.53177 val_roc= 0.89280 val_ap= 0.91392 time= 0.55056\n",
      "Epoch: 0120 train_loss= 0.43099 train_acc= 0.53213 val_roc= 0.89293 val_ap= 0.91405 time= 0.57346\n",
      "Epoch: 0121 train_loss= 0.43075 train_acc= 0.53208 val_roc= 0.89264 val_ap= 0.91440 time= 0.67532\n",
      "Epoch: 0122 train_loss= 0.43080 train_acc= 0.53204 val_roc= 0.89280 val_ap= 0.91445 time= 0.41432\n",
      "Epoch: 0123 train_loss= 0.43060 train_acc= 0.53304 val_roc= 0.89309 val_ap= 0.91407 time= 0.44608\n",
      "Epoch: 0124 train_loss= 0.43079 train_acc= 0.53272 val_roc= 0.89251 val_ap= 0.91404 time= 0.42142\n",
      "Epoch: 0125 train_loss= 0.43041 train_acc= 0.53310 val_roc= 0.89346 val_ap= 0.91473 time= 0.45190\n",
      "Epoch: 0126 train_loss= 0.43044 train_acc= 0.53297 val_roc= 0.89410 val_ap= 0.91530 time= 0.72959\n",
      "Epoch: 0127 train_loss= 0.43044 train_acc= 0.53363 val_roc= 0.89307 val_ap= 0.91492 time= 1.08636\n",
      "Epoch: 0128 train_loss= 0.43048 train_acc= 0.53351 val_roc= 0.89406 val_ap= 0.91526 time= 0.87531\n",
      "Epoch: 0129 train_loss= 0.43006 train_acc= 0.53340 val_roc= 0.89470 val_ap= 0.91551 time= 0.67599\n",
      "Epoch: 0130 train_loss= 0.43015 train_acc= 0.53317 val_roc= 0.89388 val_ap= 0.91591 time= 0.42433\n",
      "Epoch: 0131 train_loss= 0.43015 train_acc= 0.53343 val_roc= 0.89433 val_ap= 0.91608 time= 0.44296\n",
      "Epoch: 0132 train_loss= 0.42982 train_acc= 0.53386 val_roc= 0.89431 val_ap= 0.91547 time= 0.38564\n",
      "Epoch: 0133 train_loss= 0.42961 train_acc= 0.53388 val_roc= 0.89377 val_ap= 0.91540 time= 0.48918\n",
      "Epoch: 0134 train_loss= 0.42969 train_acc= 0.53387 val_roc= 0.89519 val_ap= 0.91699 time= 0.37389\n",
      "Epoch: 0135 train_loss= 0.42932 train_acc= 0.53479 val_roc= 0.89546 val_ap= 0.91713 time= 0.47626\n",
      "Epoch: 0136 train_loss= 0.42965 train_acc= 0.53479 val_roc= 0.89454 val_ap= 0.91625 time= 0.37704\n",
      "Epoch: 0137 train_loss= 0.42934 train_acc= 0.53555 val_roc= 0.89544 val_ap= 0.91686 time= 0.52851\n",
      "Epoch: 0138 train_loss= 0.42944 train_acc= 0.53529 val_roc= 0.89573 val_ap= 0.91735 time= 0.70306\n",
      "Epoch: 0139 train_loss= 0.42963 train_acc= 0.53499 val_roc= 0.89505 val_ap= 0.91748 time= 0.56137\n",
      "Epoch: 0140 train_loss= 0.42903 train_acc= 0.53636 val_roc= 0.89495 val_ap= 0.91721 time= 0.39832\n",
      "Epoch: 0141 train_loss= 0.42897 train_acc= 0.53576 val_roc= 0.89546 val_ap= 0.91681 time= 0.37856\n",
      "Epoch: 0142 train_loss= 0.42932 train_acc= 0.53505 val_roc= 0.89466 val_ap= 0.91698 time= 0.43096\n",
      "Epoch: 0143 train_loss= 0.42913 train_acc= 0.53545 val_roc= 0.89425 val_ap= 0.91704 time= 0.48825\n",
      "Epoch: 0144 train_loss= 0.42877 train_acc= 0.53596 val_roc= 0.89456 val_ap= 0.91677 time= 0.40246\n",
      "Epoch: 0145 train_loss= 0.42905 train_acc= 0.53537 val_roc= 0.89482 val_ap= 0.91734 time= 0.50636\n",
      "Epoch: 0146 train_loss= 0.42914 train_acc= 0.53549 val_roc= 0.89435 val_ap= 0.91689 time= 0.39403\n",
      "Epoch: 0147 train_loss= 0.42860 train_acc= 0.53661 val_roc= 0.89453 val_ap= 0.91694 time= 0.64900\n",
      "Epoch: 0148 train_loss= 0.42865 train_acc= 0.53690 val_roc= 0.89462 val_ap= 0.91763 time= 1.02855\n",
      "Epoch: 0149 train_loss= 0.42875 train_acc= 0.53612 val_roc= 0.89394 val_ap= 0.91663 time= 1.15049\n",
      "Epoch: 0150 train_loss= 0.42849 train_acc= 0.53695 val_roc= 0.89416 val_ap= 0.91706 time= 0.53931\n",
      "Epoch: 0151 train_loss= 0.42829 train_acc= 0.53766 val_roc= 0.89381 val_ap= 0.91741 time= 0.38645\n",
      "Epoch: 0152 train_loss= 0.42851 train_acc= 0.53735 val_roc= 0.89377 val_ap= 0.91666 time= 0.40625\n",
      "Epoch: 0153 train_loss= 0.42853 train_acc= 0.53694 val_roc= 0.89491 val_ap= 0.91738 time= 0.41715\n",
      "Epoch: 0154 train_loss= 0.42815 train_acc= 0.53728 val_roc= 0.89379 val_ap= 0.91730 time= 0.43349\n",
      "Epoch: 0155 train_loss= 0.42815 train_acc= 0.53786 val_roc= 0.89218 val_ap= 0.91619 time= 0.67194\n",
      "Epoch: 0156 train_loss= 0.42833 train_acc= 0.53789 val_roc= 0.89427 val_ap= 0.91759 time= 0.61342\n",
      "Epoch: 0157 train_loss= 0.42826 train_acc= 0.53714 val_roc= 0.89408 val_ap= 0.91713 time= 0.56766\n",
      "Epoch: 0158 train_loss= 0.42787 train_acc= 0.53852 val_roc= 0.89262 val_ap= 0.91664 time= 0.40178\n",
      "Epoch: 0159 train_loss= 0.42771 train_acc= 0.53884 val_roc= 0.89319 val_ap= 0.91720 time= 0.50205\n",
      "Epoch: 0160 train_loss= 0.42822 train_acc= 0.53763 val_roc= 0.89307 val_ap= 0.91685 time= 0.38601\n",
      "Epoch: 0161 train_loss= 0.42812 train_acc= 0.53881 val_roc= 0.89425 val_ap= 0.91798 time= 0.37869\n",
      "Epoch: 0162 train_loss= 0.42754 train_acc= 0.53903 val_roc= 0.89340 val_ap= 0.91747 time= 0.37737\n",
      "Epoch: 0163 train_loss= 0.42778 train_acc= 0.53833 val_roc= 0.89181 val_ap= 0.91643 time= 0.53323\n",
      "Epoch: 0164 train_loss= 0.42797 train_acc= 0.53941 val_roc= 0.89412 val_ap= 0.91821 time= 0.55132\n",
      "Epoch: 0165 train_loss= 0.42786 train_acc= 0.53895 val_roc= 0.89338 val_ap= 0.91754 time= 0.60345\n",
      "Epoch: 0166 train_loss= 0.42741 train_acc= 0.53917 val_roc= 0.89146 val_ap= 0.91600 time= 0.88249\n",
      "Epoch: 0167 train_loss= 0.42788 train_acc= 0.53897 val_roc= 0.89338 val_ap= 0.91796 time= 1.43952\n",
      "Epoch: 0168 train_loss= 0.42721 train_acc= 0.53962 val_roc= 0.89381 val_ap= 0.91780 time= 1.87497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0169 train_loss= 0.42719 train_acc= 0.54003 val_roc= 0.89293 val_ap= 0.91716 time= 1.22417\n",
      "Epoch: 0170 train_loss= 0.42722 train_acc= 0.54079 val_roc= 0.89255 val_ap= 0.91731 time= 0.89257\n",
      "Epoch: 0171 train_loss= 0.42734 train_acc= 0.53958 val_roc= 0.89356 val_ap= 0.91777 time= 0.87076\n",
      "Epoch: 0172 train_loss= 0.42726 train_acc= 0.54009 val_roc= 0.89309 val_ap= 0.91729 time= 0.80169\n",
      "Epoch: 0173 train_loss= 0.42712 train_acc= 0.54057 val_roc= 0.89239 val_ap= 0.91701 time= 0.71421\n",
      "Epoch: 0174 train_loss= 0.42689 train_acc= 0.54066 val_roc= 0.89388 val_ap= 0.91809 time= 0.59078\n",
      "Epoch: 0175 train_loss= 0.42698 train_acc= 0.53990 val_roc= 0.89324 val_ap= 0.91753 time= 0.53998\n",
      "Epoch: 0176 train_loss= 0.42686 train_acc= 0.54112 val_roc= 0.89319 val_ap= 0.91768 time= 0.61857\n",
      "Epoch: 0177 train_loss= 0.42668 train_acc= 0.54222 val_roc= 0.89258 val_ap= 0.91734 time= 0.78818\n",
      "Epoch: 0178 train_loss= 0.42711 train_acc= 0.54071 val_roc= 0.89323 val_ap= 0.91757 time= 0.79231\n",
      "Epoch: 0179 train_loss= 0.42661 train_acc= 0.54119 val_roc= 0.89342 val_ap= 0.91760 time= 0.81484\n",
      "Epoch: 0180 train_loss= 0.42708 train_acc= 0.54052 val_roc= 0.89354 val_ap= 0.91826 time= 0.74775\n",
      "Epoch: 0181 train_loss= 0.42666 train_acc= 0.54095 val_roc= 0.89160 val_ap= 0.91668 time= 0.68348\n",
      "Epoch: 0182 train_loss= 0.42656 train_acc= 0.54209 val_roc= 0.89229 val_ap= 0.91673 time= 0.72982\n",
      "Epoch: 0183 train_loss= 0.42700 train_acc= 0.54145 val_roc= 0.89354 val_ap= 0.91798 time= 1.17811\n",
      "Epoch: 0184 train_loss= 0.42651 train_acc= 0.54124 val_roc= 0.89208 val_ap= 0.91730 time= 1.28692\n",
      "Epoch: 0185 train_loss= 0.42640 train_acc= 0.54113 val_roc= 0.89185 val_ap= 0.91641 time= 1.55902\n",
      "Epoch: 0186 train_loss= 0.42639 train_acc= 0.54251 val_roc= 0.89324 val_ap= 0.91756 time= 0.95248\n",
      "Epoch: 0187 train_loss= 0.42667 train_acc= 0.54096 val_roc= 0.89134 val_ap= 0.91629 time= 0.57934\n",
      "Epoch: 0188 train_loss= 0.42688 train_acc= 0.54105 val_roc= 0.89262 val_ap= 0.91738 time= 0.47400\n",
      "Epoch: 0189 train_loss= 0.42648 train_acc= 0.54194 val_roc= 0.89290 val_ap= 0.91741 time= 0.74091\n",
      "Epoch: 0190 train_loss= 0.42651 train_acc= 0.54145 val_roc= 0.89113 val_ap= 0.91566 time= 0.76667\n",
      "Epoch: 0191 train_loss= 0.42685 train_acc= 0.54157 val_roc= 0.89225 val_ap= 0.91692 time= 0.47539\n",
      "Epoch: 0192 train_loss= 0.42711 train_acc= 0.54103 val_roc= 0.89227 val_ap= 0.91684 time= 0.74300\n",
      "Epoch: 0193 train_loss= 0.42631 train_acc= 0.54210 val_roc= 0.89103 val_ap= 0.91601 time= 0.53314\n",
      "Epoch: 0194 train_loss= 0.42595 train_acc= 0.54318 val_roc= 0.89235 val_ap= 0.91704 time= 0.69921\n",
      "Epoch: 0195 train_loss= 0.42680 train_acc= 0.54164 val_roc= 0.89138 val_ap= 0.91593 time= 0.95940\n",
      "Epoch: 0196 train_loss= 0.42660 train_acc= 0.54104 val_roc= 0.89111 val_ap= 0.91598 time= 0.69071\n",
      "Epoch: 0197 train_loss= 0.42580 train_acc= 0.54328 val_roc= 0.89119 val_ap= 0.91647 time= 0.96773\n",
      "Epoch: 0198 train_loss= 0.42591 train_acc= 0.54313 val_roc= 0.89109 val_ap= 0.91606 time= 0.50094\n",
      "Epoch: 0199 train_loss= 0.42603 train_acc= 0.54176 val_roc= 0.89097 val_ap= 0.91569 time= 0.74032\n",
      "Epoch: 0200 train_loss= 0.42573 train_acc= 0.54277 val_roc= 0.89191 val_ap= 0.91654 time= 1.00471\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9165704625045284\n",
      "Test AP score: 0.927927009715923\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "    # Run single weight update\n",
    "    outs = sess.run([opt.opt_op, opt.cost, opt.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_cost = outs[1]\n",
    "    avg_accuracy = outs[2]\n",
    "\n",
    "    roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)\n",
    "    val_roc_score.append(roc_curr)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "          \"train_acc=\", \"{:.5f}\".format(avg_accuracy), \"val_roc=\", \"{:.5f}\".format(val_roc_score[-1]),\n",
    "          \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "roc_score, ap_score = get_roc_score(test_edges, test_edges_false)\n",
    "print('Test ROC score: ' + str(roc_score))\n",
    "print('Test AP score: ' + str(ap_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the adjacency reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings\n",
    "feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
    "feed_dict.update({placeholders['dropout']: 0})\n",
    "emb = sess.run(model.z_mean, feed_dict=feed_dict)\n",
    "adj_pred = np.dot(emb, emb.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(3 * 50, 50))\n",
    "im1 = ax1.imshow(adj_train.toarray())\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "im2 = ax2.imshow(scipy.special.expit(adj_pred))\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "im3 = ax3.imshow((adj_pred > 0).astype(np.float_))\n",
    "plt.colorbar(im3, ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings\n",
    "feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
    "feed_dict.update({placeholders['dropout']: 0})\n",
    "emb = sess.run(model.z_mean, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downscale the embeddings\n",
    "mds = MDS(n_jobs=-2)\n",
    "mds.fit(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ground truth labels\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "ty = pickle.load(open(\"data/ind.cora.ty\", 'rb'), encoding='latin1')\n",
    "ally = pickle.load(open(\"data/ind.cora.ally\", 'rb'), encoding='latin1')\n",
    "test_idx_reorder = parse_index_file(\"data/ind.cora.test.index\")\n",
    "test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "labels = sp.vstack((ally, ty)).toarray()\n",
    "labels[test_idx_reorder, :] = labels[test_idx_range, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn labels into colors\n",
    "palette = sb.color_palette(n_colors=labels.shape[1])\n",
    "colors = np.array(palette)[np.argmax(labels, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the downscaled embeddings and the links\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "edges = np.array([[mds.embedding_[i], mds.embedding_[j]] for (i, j) in sparse_to_tuple(sp.triu(adj))[0]])\n",
    "edges = edges.transpose([2, 1, 0])\n",
    "ax.plot(edges[0], edges[1], color='lightgrey', zorder=1)\n",
    "ax.scatter(mds.embedding_[:, 0], mds.embedding_[:, 1], c=colors, zorder=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
